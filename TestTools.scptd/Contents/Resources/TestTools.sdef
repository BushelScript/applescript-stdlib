<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE dictionary SYSTEM "file://localhost/System/Library/DTDs/sdef.dtd">
<dictionary title="">
	
	<!-- note: this SDEF uses four-char codes containing "high-ASCII" chars (i.e. bytes in range 128-255 represented as characters originally found in MacOSRoman charset) to reduce the likelihood of keyword collisions with the code being tested; TO DO: check how these raw codes appear in AS when host system uses different primary encodings -->
	
	
	
	<suite name="Assertions" code="****" description="commands used to compare expected vs actual results">
	
		<!-- TO DO: slight problem using `is` as parameter name is that AS parses it differently depending on whether or not the command's parameters appear in the same order as below; if not, the `is` will be parsed as an operator instead of parameter name -->
				
		<command name="assert test result" code="Ü†ë:AsRe" description="checks and reports that test code produces the correct output for valid input">
			<parameter name="for" code="Valu" type="any" description="the actual result returned by the code being tested"/>
			<parameter name="is" code="Equa" type="any" optional="yes" description="the expected result"/>
			<parameter name="using" code="Usin" type="script" optional="yes" description="the type of check to perform (default: exact equality check)"/>
			<parameter name="note" code="Summ" type="text" optional="yes" description="any additional information to include in the test result"/>
			<documentation>
				<html><![CDATA[
<p>For example:</p>

<pre><code>to test_uppercaseText
  assert test result for (uppercase text "foøbår") is "FOØBÅR"
end test_uppercaseText</code></pre>

<p>If the <code>note</code> parameter is given, its text is included in the generate test report. For example, an <code>assert test result</code> command could use this parameter to describe the type of bugs that particular test is designed to detect.</p>
				]]></html>
			</documentation>

		</command>
	
		<command name="assert test error" code="Ü†ë:AsEr" description="checks and reports that test code throws the correct error for invalid input">
			<parameter name="in" code="Hand" type="text" optional="yes" description="the ‘call_NAME’ handler to use; if omitted, a handler with the same NAME as the current ‘test_NAME’ handler is called"/>
			<parameter name="for" type="any" code="Args" optional="yes" description="if given, the parameter to pass to the ‘call_NAME’ handler; if omitted, no parameters are passed"/>
			<parameter name="is" code="Equa" type="expected error information" description="the expected error"/>
			<parameter name="using" code="Usin" type="script" optional="yes" description="the type of check to perform (default: exact error check)"/>
			<parameter name="note" code="Summ" type="text" optional="yes" description="any additional information to include in the test result"/>
			<documentation>
				<html><![CDATA[
<p>For example:</p>

<pre><code>to test_uppercaseText
  assert test error for {a:"foo"} is {errorNumber:-1703} ¬
      note "Check unsuitable value types are rejected."
end test_uppercaseText

to call_uppercaseText(usingParam)
  uppercase text usingParam
end call_uppercaseText
</code></pre>
				]]></html>
			</documentation>
		</command>
		
		
		<record-type name="expected error information" code="Ü†ëE" description="record type used by the ‘assert…’ command to describe the expected error information"> <!-- note: this user-supplied record is distinct to the error info records TestLib uses internally for reporting failed tests; while both use the same identifier-based keys, internal error info records have their own `class` property and «class Ü†ëé» type which TestReport checks when determining how to convert test data to report text -->
			<documentation>
				<html><![CDATA[
<pre><code>{ errorNumber : <var>integer</var>,
  errorMessage : <var>text</var>,
  fromValue : <var>any</var>,
  toType : <var>type</var>,
  partialResult : <var>any</var> }</code></pre>

<p>The <var>errorNumber</var> property is required; other properties are recommended where appropriate.</p>
				]]></html>
			</documentation>
		</record-type>
		
		
	
		<command name="assert test passed" code="Ü†ë:AsPa" description="report a passed assertion">
			<parameter name="for" code="Valu" type="any" optional="yes" description="the actual result"/>
			<parameter name="note" code="Summ" type="text" optional="yes" description="any additional information to include in the test result"/>
			<documentation>
				<html><![CDATA[
<p>While <code>assert test result</code> is the preferred choice for checking whether or not a result value is correct, <code>assert test passed</code> and <code>assert test failed</code> may occasionally be used in situations that require complex checking logic beyond the capabilities of a standard ‘check’ object.</p>
				]]></html>
			</documentation>
		</command>
	
		<command name="assert test failed" code="Ü†ë:AsFa" description="report a failed assertion and abort the current test">
			<parameter name="for" code="Valu" type="any" optional="yes" description="the actual result"/>
			<parameter name="note" code="Summ" type="text" optional="yes" description="any additional information to include in the test result"/>
		</command>
		
		
	</suite>
	
	
	<suite name="Result Comparators" code="****" description="customize how expected vs actual results are compared">
	
	
		<command name="exact equality check" code="Ü†ë:ExEq" description="used in ‘assert test result’ to check that the expected and actual results are identical in both class and content">
			<result type="script"/>
		</command>
	
		<command name="numeric equality check" code="Ü†ë:NmEq" description="used in ‘assert test result’ to check that the expected and actual results are the same number, ignoring any slight differences (±1.0e-9) caused by the limited precision of real numbers">
			<parameter name="matching types" code="ETyp" type="boolean" optional="yes" description="if true, test will automatically fail if expected and actual numbers aren’t the same type; if false, test will pass as long as both numbers are numerically equal (default: true)"/>
			<result type="script"/>
			<documentation>
				<html><![CDATA[
<p>For example, the following assertion would normally fail due to the calculation <code>0.7 * 0.7</code> returning a real (a.k.a. floating point) number that is very nearly but not <em>precisely</em> 0.49:</p>

<pre><code>assert test result for (0.7 * 0.7) is 0.49 -- fails even though it shouldn’t</code></pre>

<p>To allow for the inherent imprecision of real numbers, pass a <code>NumericEqualityCheck</code> object as the <code>assert test result</code> command’s <code>using</code> parameter:</p>

<pre><code>assert test result for (0.7 * 0.7) is 0.49 using (numeric equality check) -- passes</code></pre>
				]]></html>
			</documentation>
		</command>
	

		<command name="numeric range check" code="Ü†ë:NmRg" description="used in ‘assert test result’ to check that the actual result is within the expected numeric range">
			<parameter name="matching types" code="ETyp" type="boolean" optional="yes" description="if true, test will automatically fail if expected and actual numbers aren’t the same type; if false, test will pass as long as the number is in range (default: true)"/>
			<result type="script"/>
			<documentation>
				<html><![CDATA[
<p>The <code>assert test result</code> command’s <code>is</code> parameter should be a two-number list of form <code>{<var>MIN</var>, <var>MAX</var>}</code>. For example:</p>

<pre><code>assert test result for <var>aResult</code> is {0.49, 0.51} -- result must be 0.5±0.01</code></pre>
				]]></html>
			</documentation>
		</command>
		
	
		<command name="multiple choice check" code="Ü†ë:ExMu" description="used in ‘assert test result’/‘assert test error’ to check that the actual result matches one of the items a list of expected values">
			<parameter name="using" code="Usin" type="script" optional="yes" description="the result/error check object that should be used to compare the actual result against each of the expected items in turn (default: exact equality check)"/>
			<result type="script"/>
		</command>
		
		
		<command name="exact error check" code="Ü†ë:ExEr" description="used in ‘assert test error’ to check that the actual error that occurs exactly matches the expected error information">
					<parameter name="using" code="Usin" type="record" optional="yes" description="a record of zero or more objects for checking specific error attributes (default: expected error attributes are checked for exact equality with actual error)"/>
			<result type="script"/>
			<documentation>
				<html><![CDATA[
<p>By default, each error property specified in the <code>assert test error</code> command’s <code>is</code> property is checked for exact equality with the corresponding property in the actual error. Occasionally, it may be necessary to customize the way in which a particular error property is compared, in which case the an alternate check object may be specfied to check that property.</p>

<p>To illustrate, the following <code>assert test error</code> command makes two checks: 1. that the error number is exactly 3000, and 2. that the error value is 0.49 allowing for any slight differences due to real numbers’ limited precision:</p>

<pre><code>assert test error is {errorNumber:3000 fromValue:0.49} ¬
        using {fromValue:numeric equality check}</code></pre>

<p>Thus, if the error value was produced by the calculation <code>0.7 * 0.7</code> – which returns a real number that is very nearly but not <em>precisely</em> 0.49 – this test will pass as expected.</p>
				]]></html>
			</documentation>
		</command>
	
	</suite>


	
	<suite name="Test Runner" code="****" description="commands used to interact with ‘osatest’ runner">
		
		<command name="unit test runner" code="Ü†ë:TRPa" description="returns the location of the unit test runner (a command line executable embedded in the TestTools library)">
			<result type="file"/>
		</command>
		
		<command name="run unit tests" code="Ü†ë:RunT" description="run the unit tests in the specified script file">
			<direct-parameter type="file"/>
			<result type="text" description="the test report"/>
			<documentation>
				<html><![CDATA[
<p>To run a unit test script directly in Script Editor, include the following handler in the script:</p>

<pre><code>on run
  run unit tests (path to me)
end run</code></pre>

				]]></html>
			</documentation>

		</command>
		
	</suite>


	
	<suite name="Writing Unit Tests" code="****">
		<documentation>
			<html><![CDATA[
<h3>Test Script Structure</h3>
<p>A unit test script is saved as a compiled <code><var>TESTNAME</var>.unittest.scpt</code>file and has the following basic structure:</p>

<pre><code>use script "TestTools"

use script "<var>FILENAME</var>" -- the script being tested

script suite_<var>SUITE</var> -- a group of related tests

  to test_<var>HANDLER</var>()
    -- assertions to test a command
  end

  to call_<var>HANDLER</var>(<var>PARAM</var>)
    -- the command being tested (error checking only)
  end

end</code></pre>

<p>A test script must contain one or more <code>suite_<var>NAME</var></code> script objects, each of which contains one or more <code>test_<var>NAME</var></code> handlers. As a rule of thumb, each test handler should thoroughly test a single command, asserting it returns the correct result/error responses for one or more sets of good/bad inputs.</p>

<p>Each suite object’s name must have a <code>suite_</code> prefix; each test handler’s name must have a <code>test_</code> prefix. The rest of each name can be anything: short descriptions of the purpose of the suite and the command being tested are strongly recommended as these will appear in the generated test report. When the <code>osatest</code> test runner loads the script, it searches for these <code>suite_</code> and <code>test_</code> prefixes and will invoke each test handler automatically; the script should not call test handlers itself.</p>

<p>To minimize the risk of unintended interactions between tests, prior to calling each test handler, <code>osatest</code> creates a new component instance and loads a fresh copy of the test script into it. This ensures text item delimiters, loaded libraries, etc. are not shared between tests. If the code being tested (or the tests themselves) also interact with external data – shared files, scriptable applications, etc. — it is the test script’s responsibility to ensure that these external data sources are put into a known state prior to running each test.</p>

<h3>Additional Suite Handlers</h3>

<p>In addition to <code>test_<var>NAME</var></code> handlers, a suite object may also contain zero or more of the following <em>optional</em> handlers:</p>

<dl>
<dt><code>call_<var>HANDLER</var>(...)</code></dt>
<dd><p>Used by <code>assert test error</code> commands to invoke the code being tested and capture the error raised. See the Writing Test Handlers section below.</p></dd>

<dt><code>configure_setUp()</code></dt>
<dd><p>Called by TestTools before it calls a <code>test_<var>NAME</var></code> handler in a suite. Use this handler to perform any pre-test preparation of common test data; for example, to create AppleScript objects, temporary files, database connections, etc. that will be used by each test in that suite. If a set-up handler throws an error (e.g. if a permissions error prevents a temp file being written) the rest of the test is aborted, including any post-test cleanup, in which case the set-up handler should perform its own cleanup of any partially created test resources.</p></dd>

<dt><code>configure_tearDown()</code></dt>
<dd><p>Called by TestTools after it calls a <code>test_<var>NAME</var></code> handler in a suite. Use this handler to perform any post-test cleanup; for example, to delete temporary files, close database connections, etc. This handler is called regardless of whether the test handler succeeds or fails, so should be designed to clean up any partially consumed or unused test data left by a failed test, as well as to clean up after a successful test.</p></dd>

<dt><code>configure_skipTests()</code></dt>
<dd><p>Called before running each suite. Use this handler to temporarily disable selected test handlers (e.g. if a test should only run on newer OS versions), or even the entire suite:</p>
	<ul>
		<li>If the handler returns <code>missing value</code>, all tests are run.</li>
		<li>If the handler returns a text value, no tests are run. The text should explain why the tests were skipped; this will appear in the test report.</li>
		<li>If the handler contains a record of form <code>{test_<var>NAME</var>: text or missing value, ...}</code>, the named handlers will either be skipped or run according to the property value. Test handlers not named in the record will run as normal.</p></li>
	</ul>
	
	<p>For example, to skip the test handler named <code>test_Foo</code> if the OS version is older than 10.9 while allowing all other tests in the suite to run normally:</p>

	<pre><code>use scripting additions

to configure_skipTests()
  considering numeric strings
    return {test_Foo:(system info)'s system version < "10.9"}
  end considering
end configure_doTest</code></pre>
</dd>

<dt><code>configure_doTest(testObject)</code></dt>
<dd><p>By default, TestTools calls each <code>test_<var>NAME</var></code> directly, as soon as the <code>configure_setUp()</handler> (if any) returns. Some configuration options – for example, <code>considering/ignoring</code> blocks – cannot be applied by set-up and tear-down handlers, but must instead be applied directly to the <code>test_<var>NAME</code> call itself. To do this, add a <code>configure_doTest</code> handler to the suite that takes a script object as its sole parameter. The script object contains a <code>doTest</code> handler, which takes no parameter and returns no result. The <code>configure_doTest</code> handler should perform any additional configuration, then send the script object a <code>doTest</code> command to run the test itself. For example, to run each test in a suite using custom considering/ignoring options:</p>

	<pre><code>to configure_doTest(testObject)
  considering case, diacriticals and numeric strings ¬
        but ignoring hyphens, punctuation and white space
    testObject's doTest()
  end considering
end configure_doTest</code></pre>

<p>When the <code>doTest</code> command returns, the <code>configure_doTest</code> handler may perform any related clean up and/or additional assertion checks. If the <code>doTest</code> command raises an error, the <code>configure_doTest</code> may temporaily handle it if necessary, but must re-throw that original error when done:</p>

	<pre><code>to configure_doTest(testObject)
  -- do normal preparation here
  try
    testObject's doTest()
  on error eText number eNumber from eValue to eType
    -- do essential clean up here
    error eText number eNumber from eValue to eType -- rethrow original error
  end try
  -- do normal clean up here
end configure_doTest</code></pre>
</dd>
</dl>

<h3>Writing Test Handlers</h3>

[TO DO: finish]

If the <code>assert test error</code> command includes a <code>for</code> parameter, the value is passed to the <code>call_<var>HANDLER</var>(<var>PARAM</var>)</code> handler as its sole parameter; if no <code>for</code> parameter is given then the <code>call_<var>HANDLER</var>(<var>PARAM</var>)</code> handler receives no parameters.

			]]></html>
		</documentation>
	</suite>
		
</dictionary>

